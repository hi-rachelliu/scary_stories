{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping r/scarystories With PRAW\n",
    "\n",
    "### Table of Contents \n",
    "- [Set Up](#set-up)\n",
    "- [Determine Available Attributes to Scrape](#determine-available-attributes-to-scrape)\n",
    "- [Write Functions For Pulling Submissions and Comments](#write-functions-for-pulling-submissions-and-comments)\n",
    "- [Case Study 1: Pull 10 Hot r/scarystories Submissions](#case-study-1-pull-10-hot-rscarystories-submissions)\n",
    "- [Case Study 2: Pull Submissions By Keywords](#case-study-2-pull-submissions-by-keywords)\n",
    "- [Case Study 3: Delete Duplicate Entries in CSV](#case-study-3-delete-duplicate-entries-in-csv)\n",
    "- [Case Study 4: Pull Posts Within a Specific Time Range](#case-study-4-pull-posts-within-a-specific-time-range)\n",
    "\n",
    "### Relevant Resources Used: \n",
    "- [PRAW Docs](https://praw.readthedocs.io/en/stable/tutorials/comments.html)\n",
    "- [Sentiment Analysis Using HuggingFace](https://huggingface.co/blog/sentiment-analysis-python)\n",
    "- [Cultural Analytics with Python](https://melaniewalsh.github.io/Intro-Cultural-Analytics/04-Data-Collection/14-Reddit-Data.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up Pandas\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import praw\n",
    "\n",
    "pd.set_option('max_colwidth', 500)\n",
    "\n",
    "\n",
    "# Set up PRAW with athentication\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id = \"QErgebb-REIyaM6wsoQ-Nw\",\n",
    "    client_secret = \"PPuRFlKap6UQ4D5f9tYi_pvY68ePkA\",\n",
    "    username = \"Ok_Scientist2546\",\n",
    "    password = \"EZ8y@'ctT!f4A%L\",\n",
    "    user_agent = \"Praw-test\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Available Attributes to Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Available Attributes of a Submission object\n",
    "import pprint\n",
    "\n",
    "sub_by_id = reddit.submission(\"14828yd\")\n",
    "print(sub_by_id.title)  \n",
    "pprint.pprint(vars(sub_by_id))\n",
    "\n",
    "# Determine Available Attributes of a Comment object\n",
    "comment = list(sub_by_id.comments)[0]\n",
    "print(comment.body)  \n",
    "pprint.pprint(vars(comment))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Functions For Pulling Submissions and Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS ###\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def pull_submissions(num_subs: int, sub_name: str, sort: str, times=[], keywords=[]):\n",
    "    \"\"\"\n",
    "    Gets key details about num_subs number of submissions on a particular subreddit sub_name. \n",
    "\n",
    "\n",
    "    Inputs:\n",
    "        - num_subs [int]: the number of submissions to pull\n",
    "        - sub_name [str]: subreddit name without the r/, i.e., \"scarystories\"\n",
    "        - sort [str]: the way to sort the subreddit, i.e. by \"controversial,\" \n",
    "            \"hot,\" \"new,\" \"rising,\" or \"top\". Note that rising doesn't necessarily have as \n",
    "            many submissions as specified in num_subs.\n",
    "        - keywords [List[str]]: list of keywords to filter by. By default an empty list. At least 1 \n",
    "            keyword must appear once in the submission text for the submission to be returned\n",
    "        - times [List[str]]: the bounds of the time range you want to pull from. [past_time, recent_time]\n",
    "            both in utc form. By default, will pull from every year up to now.\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "        [List[Dict[10 items]]]: a list of dictionaries, one for each submission in the specified subreddit \n",
    "    \"\"\"\n",
    "\n",
    "    subreddit = reddit.subreddit(sub_name)\n",
    "    res = []\n",
    "    num = 0\n",
    "\n",
    "    SORTED_SUBMISSIONS = {\"hot\": subreddit.hot(), \n",
    "                          \"controversial\": subreddit.controversial(), \n",
    "                          \"gilded\": subreddit.gilded(),\n",
    "                          \"top\": subreddit.top(),\n",
    "                          \"new\": subreddit.new(),\n",
    "                          \"rising\": subreddit.rising()\n",
    "                          }\n",
    "    \n",
    "    if times:\n",
    "        past, recent = times\n",
    "    else:\n",
    "        past = datetime(2005, 6, 23).replace(tzinfo=timezone.utc).timestamp()\n",
    "        recent = datetime.now(timezone.utc).timestamp()\n",
    "\n",
    "    for submission in SORTED_SUBMISSIONS[sort]:\n",
    "        if submission.created_utc >= past and submission.created_utc < recent:\n",
    "            if keywords == [] or key_words_in_text(keywords, submission.selftext):\n",
    "                if num >= num_subs:\n",
    "                    return res\n",
    "                num += 1\n",
    "                print(\"Copying submission info...\")\n",
    "                story = {}\n",
    "                story[\"title\"] = submission.title\n",
    "                story[\"submission_id\"] = submission.id\n",
    "                story[\"score\"] = submission.score\n",
    "                story[\"url\"] = submission.url\n",
    "                story[\"author\"] = \"Deleted\" if submission.author is None else submission.author.name\n",
    "                story[\"text\"] = (submission.selftext.replace(\"’\", \"'\").\n",
    "                                replace(\"…\", \"...\").replace(\"\\n\", \" \").replace(\"“\", \"\\\"\").\n",
    "                                replace(\"”\", \"\\\"\"))\n",
    "                story[\"subreddit\"] = submission.subreddit\n",
    "                story[\"num_comments\"] = submission.num_comments\n",
    "                story[\"date_created\"] = datetime.fromtimestamp(submission.created_utc)\n",
    "                res.append(story)\n",
    "\n",
    "\n",
    "def key_words_in_text(keywords, text):\n",
    "    \"\"\"\n",
    "    Checks if any of the keywords are in the text.\n",
    "\n",
    "    Inputs:\n",
    "        keywords [List[str]]: a list of key words to check\n",
    "        text [str]: string text to check for words\n",
    "\n",
    "    Returns: True if any of the keywords are in the text, False otherwise.\n",
    "    \"\"\"\n",
    "    for word in keywords: \n",
    "        if word in text.lower(): \n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def pull_comments(subreddit_id: str, amount: str=\"all\"):\n",
    "    \"\"\"\n",
    "    Pull all or top level comments from a certain reddit submission.\n",
    "\n",
    "    Inputs:\n",
    "        subreddit_id [str]: the subreddit id of subreddit you want to pull from\n",
    "        amount [str]: how many comments to pull, all comments or only top level comments. \n",
    "            By default, this variable has value \"all\"\n",
    "\n",
    "\n",
    "    Returns: \n",
    "        [List[Dict[8 items]]]: a list of comments from a single submission with the comment details\n",
    "    \"\"\"\n",
    "\n",
    "    sub_by_id = reddit.submission(subreddit_id)\n",
    "\n",
    "    # Select top level comments or all comments \n",
    "    sub_by_id.comments.replace_more(limit=None)\n",
    "    if amount == \"top_level\":\n",
    "        comments = []\n",
    "        for top_level_comment in sub_by_id.comments:\n",
    "            comments.append(top_level_comment) \n",
    "    else:\n",
    "        comments = sub_by_id.comments.list()\n",
    "\n",
    "    # Return List of dictionaries with comment details\n",
    "    res = []\n",
    "    for comment in comments:\n",
    "        new_comment = {}\n",
    "        new_comment[\"text\"] = (comment.body.replace(\"’\", \"'\").\n",
    "                             replace(\"…\", \"...\").replace(\"\\n\", \" \").replace(\"“\", \"\\\"\").\n",
    "                            replace(\"”\", \"\\\"\"))\n",
    "        # Text needs to contain the keyword to be returned!\n",
    "        new_comment[\"author\"] = \"Deleted\" if comment.author is None else comment.author.name\n",
    "        new_comment[\"score\"] = comment.score\n",
    "        new_comment['comment_id'] = comment.id\n",
    "        new_comment[\"is_op\"] = comment.is_submitter\n",
    "        new_comment[\"submission_id\"] = comment._submission.id\n",
    "        new_comment[\"subreddit\"] = comment.subreddit_name_prefixed\n",
    "        new_comment[\"subreddit_id\"] = comment.subreddit_id\n",
    "        res.append(new_comment)\n",
    "    return res\n",
    "\n",
    "\n",
    "def write_to_csv(obj, file, mode=\"a\"):\n",
    "     \"\"\"\n",
    "     Writes info from a List[Dict[items]] object into a csv file.\n",
    "\n",
    "     Inputs:\n",
    "        mode [str]: \"w\" for write or \"a\" for append. By default, \"a\" for append.\n",
    "        obj [List[Dict[items]]]: the object that contains the info to write\n",
    "        file [str]: csv file name\n",
    "\n",
    "     Returns:\n",
    "        Nothing\n",
    "     \"\"\"\n",
    "     fieldnames = obj[0].keys()\n",
    "     with open(file, mode, newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if mode == \"w\":\n",
    "            writer.writeheader()\n",
    "        writer.writerows(obj)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 1: Pull 10 Hot r/scarystories Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_ten_stories = pull_submissions(num_subs=10, sub_name=\"scarystories\", sort=\"hot\")\n",
    "write_to_csv(hot_ten_stories, \"submissions.csv\", \"w\")\n",
    "stories_df = pd.read_csv(\"submissions.csv\", delimiter=',', encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 2: Pull Submissions By Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_50 = pull_submissions(num_subs=50, sub_name=\"scarystories\", keywords=[\"scary\", \"stab\", \"kill\"])\n",
    "write_to_csv(subs_50, 'submissions.csv', \"w\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 3: Delete Duplicate Entries in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull first 50\n",
    "subs_50 = pull_submissions(num_subs=50, sub_name=\"scarystories\", sort=\"hot\", keywords=[\"scary\", \"stab\", \"kill\"])\n",
    "write_to_csv(subs_50, 'submissions.csv', \"w\")\n",
    "df = pd.read_csv(\"submissions.csv\", delimiter=',', encoding='utf-8')\n",
    "df\n",
    "\n",
    "# Pull another 50\n",
    "other_50 = pull_submissions(num_subs=50, sort=\"new\", sub_name=\"scarystories\")\n",
    "write_to_csv(other_50, 'submissions.csv')\n",
    "\n",
    "# Delete duplicate entries\n",
    "temp = pd.read_csv('submissions.csv')\n",
    "pd.concat([temp, df]).drop_duplicates().to_csv('submissions.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 4: Pull Posts Within a Specific Time Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Only pull between a certain time range\n",
    "\n",
    "from datetime import timezone\n",
    "\n",
    "current_time = datetime.now(timezone.utc).timestamp()\n",
    "one_year_ago = datetime(2022, 1, 1).replace(tzinfo=timezone.utc).timestamp()\n",
    "eight_years_ago = datetime(2015, 1, 1).replace(tzinfo=timezone.utc).timestamp()\n",
    "\n",
    "# Pull 20 submissions from between eight years ago and one year ago\n",
    "after_twentyfifteen = pull_submissions(num_subs=20, sub_name=\"scarystories\", sort=\"hot\", times=[eight_years_ago, one_year_ago])\n",
    "print(after_twentyfifteen)\n",
    "\n",
    "# this won't work with PRAW!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 5: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: {'label': 'NEG', 'score': 0.9753668904304504}\n",
      "1: {'label': 'NEU', 'score': 0.6773768067359924}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "comment = \"https://www.reddit.com/r/sysadmin/comments/11egyx5/comment/jahuiqz/\"\n",
    "subreddit = reddit.subreddit(\"sysadmin\")\n",
    "submission = reddit.submission(\"11egyx5\")\n",
    "text1 = \"The entire history of Firepower could easily be viewed as a graduate school case study in how clueless executives can correctly identify a product gap, spend the right money on the right technology to close that gap, and piss all of the value away through mismanagement anyway.\"\n",
    "text2 = \"Cisco knew PIX was beyond it's usable service-life, and original ASA-OS was weak. Cisco knew they needed a new Layer-7-aware Firewall solution to compete with Palo & Fortinet and CheckPoint. In 2013 Cisco pulled out the BIG checkbook and bought the OG, mac-daddy, gold-standard L7 aware security solution: Snort.\"\n",
    "data = []\n",
    "data.append(text1)\n",
    "data.append(text2)\n",
    "\n",
    "\n",
    "specific_model = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "sentiments = specific_model(data)\n",
    "for i, senti in enumerate(sentiments):\n",
    "    print(f\"{i}: {senti}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to Consider\n",
    "\n",
    "- TODO: filter pull_comments() by keywords too\n",
    "- Might need to go through csv manually to make sure that keyword is not accidental -- probably less lilely if keyword is \"Cisco\", but more likely with something like \"stab\"\n",
    "- Duplicates might not include same post with slightly different stats - need to sort out manually if so\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
